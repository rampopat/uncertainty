{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Performance Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1unnFVDG1F6nxLhV4VPAgR7lrcJKWS6ZF",
      "authorship_tag": "ABX9TyPcmk8zVbRhVcKuT3qGglTR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rampopat/uncertainty/blob/main/Performance_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTIBqzwkYvzE"
      },
      "source": [
        "# Performance Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYb4W-KTYzvq"
      },
      "source": [
        "This notebook contains the performance analysis for both datasets. Broadly, we have the imports then the key performance analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voSA17yxZdiH"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbXXjUR_Y7XL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0FMHU9zf8tZ"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, roc_curve, plot_roc_curve, confusion_matrix, classification_report\n",
        "from sklearn.calibration import calibration_curve\n",
        "import sklearn.metrics\n",
        "from scipy.stats import entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-ESP5ordyh9"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWZQ9R4S2JDf"
      },
      "source": [
        "!pip install netcal\n",
        "import netcal as nc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsv2p_9stz8p"
      },
      "source": [
        "from netcal.scaling import TemperatureScaling\n",
        "from netcal.metrics import ECE\n",
        "from netcal.presentation import ReliabilityDiagram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV9mNzzcrH2A"
      },
      "source": [
        "import warnings\n",
        "import sklearn\n",
        "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSwBg6cc-d5a"
      },
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ2DUlpoFsDN"
      },
      "source": [
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-4rtbGq6P4Q"
      },
      "source": [
        "!unzip drive/MyDrive/UncertaintyAnalysisData.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtGINDlbpUpE"
      },
      "source": [
        "# Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7L5tbgDpgyE"
      },
      "source": [
        "def binarize(ps, threshold=0.5):\n",
        "    return np.where(ps > threshold, 1, 0)\n",
        "\n",
        "def ece(y_true, y_pred, bins=30):\n",
        "    # This is a pmetric because we want to calculate this on portions of data rather than order data by it\n",
        "    # Use same function signature as the other pmetrics to allow usage\n",
        "    ece = ECE(bins)\n",
        "    return 1 - ece.measure(y_pred, y_true)\n",
        "\n",
        "def ece_ts(y_true, y_pred, bins=30):\n",
        "    # This is a pmetric because we want to calculate this on portions of data rather than order data by it\n",
        "    # Use same function signature as the other pmetrics to allow usage\n",
        "    temperature = TemperatureScaling()\n",
        "    temperature.fit(y_pred, y_true)\n",
        "    calibrated = temperature.transform(y_pred)\n",
        "    ece = ECE(bins)\n",
        "    return 1 - ece.measure(calibrated, y_true)\n",
        "\n",
        "PATH = 'UncertaintyAnalysisData/'  \n",
        "\n",
        "def get_df(name1, name2):    \n",
        "    df = pd.read_pickle(PATH + name1).sort_index()\n",
        "    all_preds = pd.read_pickle(PATH + name2).to_numpy()\n",
        "    return df, all_preds\n",
        "def get_df_from_preds(all_preds=None, name=None, labels=None, threshold=0.5):\n",
        "    if all_preds is None:\n",
        "        all_preds = pd.read_pickle(PATH + name).to_numpy()    \n",
        "    # all_preds.shape = (ensemble_size, data_size)\n",
        "    preds = all_preds.mean(axis=0)\n",
        "    df = pd.DataFrame()\n",
        "    df['pred'] = preds\n",
        "    df['pred_label'] = binarize(preds, threshold=threshold)\n",
        "    df['true_label'] = labels\n",
        "    df['correct'] = df['pred_label'] == df['true_label']\n",
        "    df = df.astype({'correct': int })\n",
        "    return df, all_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLQw_CXzpTxh"
      },
      "source": [
        "preds_rnn_mc = np.load(PATH + 'rnnmc.npy')\n",
        "labels = get_df('rnnens10.pkl', 'rnnens10ap.pkl')[0].true_label\n",
        "df_rnn_mc, preds_rnn_mc = get_df_from_preds(all_preds=np.load(PATH + 'rnnmc.npy'), labels=labels)\n",
        "df_rnn, preds_rnn = get_df_from_preds(name='rnnens10ap.pkl', labels=labels)\n",
        "# df_trf, preds_trf = get_df_from_preds(name='trf10ap.pkl', labels=labels) #tbert\n",
        "df_brnns, preds_brnns = get_df_from_preds(name='brnnens5ap.pkl', labels=labels)\n",
        "df_brnn, preds_brnn = get_df_from_preds(name='brnnap.pkl', labels=labels)\n",
        "df_trf, preds_trf = get_df_from_preds(name='troberta5ap.pkl', labels=labels)\n",
        "df_strf, preds_strf = get_df_from_preds(name='sroberta5ap.pkl', labels=labels)\n",
        "df_trf_mc, preds_trf_mc = get_df_from_preds(name='trobertamcseed3.pkl', labels=labels)\n",
        "\n",
        "some_dfs = [df_rnn, df_brnn, df_brnns, df_trf]\n",
        "SOME_DF_NAMES = ['RNN-ENS', 'BRNN', 'BRNN-ENS', 'T-TRF-ENS']\n",
        "\n",
        "dfs = [df_rnn_mc, df_rnn, df_brnn, df_brnns, df_trf_mc, df_trf, df_strf]\n",
        "DF_NAMES = ['RNN-MC', 'RNN-ENS', 'BRNN', 'BRNN-ENS', 'T-TRF-MC', 'T-TRF-ENS', 'STRF-ENS']\n",
        "\n",
        "preds_ens = [preds_rnn, preds_brnns, preds_trf, preds_strf]\n",
        "dfs_ens = [df_rnn, df_brnns, df_trf, df_strf]\n",
        "DF_NAMES_ENS = ['RNN-ENS', 'BRNN-ENS', 'T-TRF-ENS', 'S-TRF-ENS']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33kj76qxhw0l"
      },
      "source": [
        "patient_num = 19\n",
        "bins=1000\n",
        "brnn_preds = preds_brnns[:, patient_num]\n",
        "rnn_preds = preds_rnn[:, patient_num]\n",
        "plt.hist(brnn_preds, 10, density=True)\n",
        "plt.hist(rnn_preds, 10, density=True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndyya5ZGL4BX"
      },
      "source": [
        "for i, preds in enumerate(preds_ens):\n",
        "    print(DF_NAMES_ENS[i])\n",
        "    print(preds.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt7YSB8jFF7m"
      },
      "source": [
        "pd.Series(np.rint(preds_rnn).mean(axis=0)).hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LRp7qSIFcWh"
      },
      "source": [
        "for i, preds in enumerate(preds_ens):\n",
        "    sns.histplot(pd.Series(np.rint(preds).mean(axis=0)))\n",
        "    plt.xlabel('Mean of Ensemble Predicted Labels')\n",
        "    plt.title(DF_NAMES_ENS[i])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi-Qy1FhIGGo"
      },
      "source": [
        "rinted_preds = [np.rint(preds).mean(axis=0) for preds in preds_ens[:-1]]\n",
        "plt.hist(rinted_preds, label=DF_NAMES_ENS[:-1])\n",
        "plt.xlabel('Mean of Ensemble Predicted Labels')\n",
        "plt.ylabel('Number of Patients')\n",
        "plt.title('Disagreement Intra-Ensemble')\n",
        "plt.legend()\n",
        "plt.savefig('disagreement-ensemble', dpi=DPI)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3zaNNJmqX5m"
      },
      "source": [
        "PMETRIC_FNS = [accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, ece, ece_ts]\n",
        "PMETRIC_NAMES = ['Accuracy', 'F1', 'ROC AUC', 'Precision', 'Recall', '1-ECE', '1-ECE+TS'] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Xwd_Vqq-rv"
      },
      "source": [
        "def compute_pmetrics(df, ret_cfm=False):\n",
        "    true_labels = df.true_label.to_numpy()\n",
        "    pred_labels = df.pred_label.to_numpy()\n",
        "    preds = df.pred.to_numpy()\n",
        "    results = []\n",
        "    for metric in PMETRIC_FNS:\n",
        "        if metric == roc_auc_score or metric == ece or metric == ece_ts:\n",
        "            result = metric(true_labels, preds)\n",
        "        else:\n",
        "            result = metric(true_labels, pred_labels)\n",
        "        results.append(result)\n",
        "    if ret_cfm: \n",
        "        return confusion_matrix(true_labels, pred_labels, normalize='all')\n",
        "    return results\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gtoDTwRQ_NQ"
      },
      "source": [
        "def compute_pmetrics_for_preds(ens_preds, labels):\n",
        "    # ens_preds.shape = (ensemble_size, data_size)\n",
        "    # rinted_preds = np.rint(ens_preds)\n",
        "    true_labels = labels.to_numpy()\n",
        "    results = []\n",
        "    for i, preds in enumerate(ens_preds):\n",
        "        # preds.shape = (data_size,)\n",
        "        model_results = []\n",
        "        for metric in PMETRIC_FNS:\n",
        "            if metric == roc_auc_score or metric == ece or metric == ece_ts:\n",
        "                result = metric(true_labels, preds)\n",
        "            else:\n",
        "                result = metric(true_labels, np.rint(preds))\n",
        "            model_results.append(result)\n",
        "        results.append(model_results.copy())\n",
        "    return np.array(results) # shape = (ensemble_size, PMETRICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQRUjwU4362v"
      },
      "source": [
        "def calibration_analysis(df):\n",
        "    confidences = df.pred.to_numpy()\n",
        "    ground_truth = df.true_label.to_numpy()\n",
        "    temperature = TemperatureScaling()\n",
        "    temperature.fit(confidences, ground_truth)\n",
        "    calibrated = temperature.transform(confidences)\n",
        "\n",
        "    n_bins = 30\n",
        "    print('ECE Scores')\n",
        "    ece = ECE(n_bins)\n",
        "    uncalibrated_score = ece.measure(confidences, ground_truth)\n",
        "    print('uncalibrated', uncalibrated_score)\n",
        "    calibrated_score = ece.measure(calibrated, ground_truth)\n",
        "    print('after temp scaling', calibrated_score)\n",
        "\n",
        "    diagram = ReliabilityDiagram(n_bins)\n",
        "    _ = diagram.plot(confidences, ground_truth) \n",
        "    _ = diagram.plot(calibrated, ground_truth) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccDE_qkt38iJ"
      },
      "source": [
        "calibration_analysis(df_rnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgWOwEFYV9PC"
      },
      "source": [
        "pmetrics = compute_pmetrics_for_preds(preds_rnn, labels)\n",
        "plt.scatter(np.arange(pmetrics.shape[0]), pmetrics[:, 0], label='Individual Scores')\n",
        "plt.axhline(y=accuracy_score(labels.to_numpy(), np.rint(preds_rnn.mean(axis=0))), color='r', linestyle='-', label='Ensembled Score')\n",
        "plt.axhline(y=pmetrics[:, 0].mean(axis=0), color='b', linestyle='-', label='Mean Score')\n",
        "plt.xlabel('Model number')\n",
        "plt.ylabel('Accuracy Score')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAIchPjwTArC"
      },
      "source": [
        "metric_num = 0 # accuracy_score\n",
        "print(PMETRIC_NAMES[metric_num]) # this is what we're getting\n",
        "results = []\n",
        "for i, preds in enumerate(preds_ens):\n",
        "    # Get metrics for individual models in the ensemble\n",
        "    # shape = (ensemble_size,)\n",
        "    metrics_per_model = compute_pmetrics_for_preds(preds, labels)[:, metric_num]\n",
        "    mean_metric = metrics_per_model.mean()\n",
        "    std_metric = metrics_per_model.std()\n",
        "    mean_pm_str = '$' + str(mean_metric.round(3)) + 'pm' + str(std_metric.round(3)) + '$'\n",
        "    print(mean_pm_str)\n",
        "    ensembled_score = compute_pmetrics(dfs_ens[i])[metric_num]\n",
        "    results.append([ensembled_score, mean_pm_str, metrics_per_model.min(), metrics_per_model.max()])\n",
        "ens_results = pd.DataFrame(results, columns=['Ensembled', 'Mean $\\pm$ Std', 'Min', 'Max']).round(3)\n",
        "ens_results.index = DF_NAMES_ENS\n",
        "ens_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiQjXlpobR70"
      },
      "source": [
        "print(ens_results.to_latex()) # it's a bit messed up "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3kkTnuvs15w"
      },
      "source": [
        "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
        "p_results = [compute_pmetrics(df) for df in dfs]\n",
        "# shape = [num_dfs, num_pmetrics]\n",
        "p_results = np.array(p_results).round(3)\n",
        "p_results = pd.DataFrame(p_results, columns=PMETRIC_NAMES)\n",
        "p_results.index = DF_NAMES\n",
        "p_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxKU9WAd8dNn"
      },
      "source": [
        "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
        "p_results = [compute_pmetrics(all_preds) for all_preds in preds_ens]\n",
        "# shape = [num_dfs, num_pmetrics]\n",
        "p_results = np.array(p_results).round(3)\n",
        "p_results = pd.DataFrame(p_results, columns=PMETRIC_NAMES)\n",
        "p_results.index = DF_NAMES\n",
        "p_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCslbfqkvbmQ"
      },
      "source": [
        "print(p_results.to_latex())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxIgsVLZ9Oqf"
      },
      "source": [
        "import re\n",
        "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
        "p_results = np.array([compute_pmetrics_for_preds(ps, labels).mean(axis=0) for ps in preds_ens]).round(3)\n",
        "# shape = [num_dfs, num_pmetrics]\n",
        "p_results = pd.DataFrame(p_results, columns=PMETRIC_NAMES)\n",
        "p_results.index = [re.sub('-ENS', '', name) for name in DF_NAMES_ENS]\n",
        "print(p_results.to_latex())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4kebqts444V"
      },
      "source": [
        "cfms = [compute_pmetrics(df, ret_cfm=True) for df in some_dfs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxusV1Zy5D_u"
      },
      "source": [
        "print(np.array(cfms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1Ci-0bl5_tq"
      },
      "source": [
        "plt.subplots(1, 4, figsize=(8, 2))\n",
        "plt.tight_layout()\n",
        "for i in range(len(cfms)):\n",
        "    plt.subplot(1, 4, i + 1)\n",
        "    make_confusion_matrix(cfms[i], count=False, categories=['HC', 'AD'], sum_stats=False, cbar=False, \n",
        "                          title=SOME_DF_NAMES[i])\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion-matrices', dpi=DPI)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGQ21RXu-h80"
      },
      "source": [
        "def get_roc_curve(df, name=''):\n",
        "    fpr, tpr, thresholds = roc_curve(df.true_label, df.pred)\n",
        "    roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
        "    label = name + ' (AUC = {:.2f})'.format(roc_auc)\n",
        "    # display = sklearn.metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=None)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.plot(fpr, tpr, label=label) \n",
        "    plt.legend()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jg3XSbX-itr"
      },
      "source": [
        "plt.figure(figsize=(5, 5))   \n",
        "for i in range(len(dfs)):\n",
        "    get_roc_curve(dfs[i], name=DF_NAMES[i])\n",
        "plt.title('ROC Curves')\n",
        "plt.savefig('roc-curves', dpi=DPI)\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LH9TsQrBNj-"
      },
      "source": [
        "def get_calibration_curve(df, name=' ', bins=10):\n",
        "    true_labels = df.true_label.to_numpy()\n",
        "    preds = df.pred.to_numpy()\n",
        "    y, x = calibration_curve(true_labels, preds, n_bins=bins)\n",
        "    plt.plot(x, y, label=name, marker='.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfxIXu0wBr5N"
      },
      "source": [
        "bins = 5\n",
        "plt.figure(figsize=(8,5))   \n",
        "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly Calibrated\")\n",
        "for i in range(len(dfs)):\n",
        "    get_calibration_curve(dfs[i], name=DF_NAMES[i], bins=bins)\n",
        "plt.title('Reliability Diagrams')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Observed Probability')\n",
        "plt.legend(bbox_to_anchor=(1.04,0.5), loc=\"center left\")\n",
        "# plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "plt.savefig('reliability-diagrams', dpi=DPI)\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}